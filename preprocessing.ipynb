{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "\n",
    "print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_origin = pd.read_csv('open/train.csv')\n",
    "\n",
    "train_df = pd.read_csv('open/train.csv')\n",
    "test_df = pd.read_csv('open/test.csv')\n",
    "sub_df = pd.read_csv('open/sample_submission.csv')\n",
    "\n",
    "print('Number of Data for training : {col}\\n Number of Variables : {row}'\n",
    "      .format(col = train_df.shape[0], row = train_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[[611,612,613,614,615]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2018.01.01 ~ 2020.12.31 (NaN of 2019.09.07 sat ;;;;;)\n",
    "# \n",
    "new_row = pd.DataFrame(train_df.loc[[613]], columns = train_df.columns)\n",
    "\n",
    "new_df = pd.concat([train_df.iloc[:613], new_row, train_df.iloc[613:]], ignore_index = True)\n",
    "\n",
    "new_df.loc[614,'date'] = '2019-09-07' \n",
    "\n",
    "\n",
    "print(new_df.loc[[612,613,614,615]])\n",
    "len(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To distinguish weekend and weekday\n",
    "# The name of new index is \"days of week\" \n",
    "# [0:Mon , 1:Tue, 2:Wen, 3:Thur, 4:Fri, 5:Sat, 6:Sun]\n",
    "\n",
    "df_week = [] # List for df column \"days of week\"\n",
    "\n",
    "L1 = new_df.shape[0] // 7  # 1096 // 7\n",
    "L2 = new_df.shape[0] % 7  # 1096 // 7\n",
    "\n",
    "#week = [0, 1, 2, 3, 4, 5, 6]\n",
    "week = [0, 0, 0, 0, 0, 1, 1] # weekday / weekend\n",
    "\n",
    "for i in range(0,L1,1):\n",
    "    df_week.extend(week)\n",
    "\n",
    "for i in range(0,L2,1):\n",
    "    df_week.append(0)\n",
    "\n",
    "print(df_week)\n",
    "print(len(df_week))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = []\n",
    "month = []\n",
    "day = [] \n",
    "\n",
    "for date in new_df[\"date\"] :\n",
    "    y = date.split('-')[0]\n",
    "    m = date.split('-')[1]\n",
    "    d = date.split('-')[2]\n",
    "    \n",
    "    year.append(y)\n",
    "    month.append(m)\n",
    "    day.append(d)\n",
    "\n",
    "new_df[\"year\"] = year \n",
    "new_df[\"month\"] = month \n",
    "new_df[\"day\"] = day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appended New Col \"days of week\"\n",
    "new_df['days_of_week'] = df_week\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SS_list_Nan= new_df['sunshine_sum'][new_df['sunshine_sum'].isnull()].index\n",
    "print(SS_list_Nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['sunshine_rate'].loc[SS_list_Nan].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[['sunshine_rate', 'sunshine_sum']].corr(method='pearson') # Pearson Correlation\n",
    "\n",
    "a = new_df['sunshine_sum'].dropna()\n",
    "b = new_df['sunshine_rate'].drop(SS_list_Nan)\n",
    "a.isnull().sum()\n",
    "b.isnull().sum()\n",
    "\n",
    "plt.plot(a,b, 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SS_list_Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.values.reshape(-1,1) # a is sum\n",
    "b = b.values.reshape(-1,1) # b is rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a)\n",
    "print(b)\n",
    "new_df['sunshine_rate'].loc[SS_list_Nan] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_fitter = LinearRegression()\n",
    "\n",
    "line_fitter.fit(b, a) # predict a(sum) through b(rate) \n",
    "SS_sol_Nan = line_fitter.predict(new_df['sunshine_rate'].loc[SS_list_Nan].values.reshape(-1,1))\n",
    "SS_sol_Nan = list(SS_sol_Nan)\n",
    "float(SS_sol_Nan[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[\"PM10\"] = new_df[\"PM10\"].fillna(new_df[\"PM10\"].mean())\n",
    "new_df[\"PM2.5\"] = new_df[\"PM2.5\"].fillna(new_df[\"PM2.5\"].mean())\n",
    "new_df[\"sunshine_sum\"] = new_df[\"sunshine_sum\"].fillna(0)\n",
    "\n",
    "\n",
    "print(new_df['sunshine_sum'].loc[SS_list_Nan])\n",
    "print(float(SS_sol_Nan[1]))\n",
    "\n",
    "for i, j in zip(SS_list_Nan, range(0,5,1)):\n",
    "    new_df['sunshine_sum'].loc[i] = float(SS_sol_Nan[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_df['sunshine_sum'].loc[SS_list_Nan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PP_list_Nan= new_df['precipitation'][new_df['precipitation'].isnull()].index\n",
    "print(PP_list_Nan)\n",
    "\n",
    "pp = new_df['precipitation'].dropna()\n",
    "srss = new_df[['sunshine_rate','sunshine_sum']].drop(PP_list_Nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pp = pp.values.reshape(-1,1) # pp is precipitation\n",
    "#srss = srss.values.reshape(-1,1) # sr is sun_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_fitter.fit(srss, pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_sol_Nan = line_fitter.predict(new_df[['sunshine_rate','sunshine_sum']].loc[PP_list_Nan].values)\n",
    "pp_sol_Nan = list(pp_sol_Nan)\n",
    "pp_sol_Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = new_df.drop(['precipitation','date','rental'], axis=1)\n",
    "y = new_df['rental']\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(x, y, test_size = 0.25, shuffle=True)\n",
    "\n",
    "test_x = test_df.drop(['date'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMAE(true, pred):\n",
    "    score = np.mean(np.abs(true-pred) / true)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = RandomForestRegressor()\n",
    "reg.fit(train_x, train_y)\n",
    "val_pred = reg.predict(val_x)\n",
    "\n",
    "NMAE(val_y, val_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = TwoLayerNet(input_size=14, hidden_size=5, output_size=2) #  Activation function : ReLU\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = train_x.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.15\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1) # max(8, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = train_x[batch_mask]\n",
    "    t_batch = train_y[batch_mask]\n",
    "    \n",
    "    # ê¸°ì¸ê¸° ê³ì°\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch) # ìì¹ ë¯¸ë¶ ë°©ì\n",
    "    grad = network.gradient(x_batch, t_batch) # ì¤ì°¨ì­ì íë² ë°©ì(í¨ì¬ ë¹ ë¥´ë¤)\n",
    "    \n",
    "    # ê°±ì \n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(train_x, train_y)\n",
    "        test_acc = network.accuracy(train_x, train_y)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('lftest')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ffb198bb7cebc8a652939ac07b28802de13ce4b22d89c3afd3bd3a1bb7233d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
